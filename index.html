<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Sekai: A Video Dataset for World Exploration">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="video generation, world model">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ùôéùôöùô†ùôñùôû</title>
  <link rel="icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/js/fontawesome.all.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/js/bulma-slider.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/hls.js@latest"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      var video = document.getElementById('hero-video');
      if (Hls.isSupported()) {
        var hls = new Hls();
        hls.loadSource('https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/videos/banner_x264.m3u8');
        hls.attachMedia(video);
        hls.on(Hls.Events.MANIFEST_PARSED, function() {
          video.play();
        });
      }
      else if (video.canPlayType('application/vnd.apple.mpegurl')) {
        video.src = 'https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/videos/banner_x264.m3u8';
        video.addEventListener('loadedmetadata', function() {
          video.play();
        });
      }
    });
  </script>
</head>

<body>


  <section class="hero is-fullheight">
    <div class="hero-video">
      <video id="hero-video" autoplay muted loop playsinline>
        <source src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/videos/banner_x264.m3u8" type="application/x-mpegURL">
      </video>
      <div class="hero-video-overlay"></div>
    </div>
    <div class="hero-body" style="padding-top: 24vh;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title publication-title has-text-white" style="font-size: 3.5rem;white-space: nowrap;"><span style="color: #6fbf73;">ùôé</span><span style="color: #007bff;">ùôö</span></span><span style="color: #ffac33;">ùô†</span><span style="color: #9976db;">ùôñ</span><span style="color: #ed4b82;">ùôû</span> : A Video Dataset for World Exploration</h1>
            <div class="is-size-4 publication-authors has-text-white">
              <span class="author-block">Zhen Li<sup style="color:#6fbf73; font-size: 0.7em;">1</sup><sup style="font-size: 0.7em;">,</sup><sup style="color:#007bff; font-size: 0.7em;">2</sup><sup style="font-size: 0.7em;">,</sup><sup style="color:#9976db; font-size: 0.7em;">4</sup>,</span>
              <span class="author-block">Chuanhao Li<sup style="color:#6fbf73; font-size: 0.7em;">1</sup><sup style="font-size: 0.7em;">,</sup><sup style="font-size: 0.7em;">üìß</sup>,</span>
              <span class="author-block">Xiaofeng Mao<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Shaoheng Lin<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Ming Li<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Shitian Zhao<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Zhaopan Xu<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span><br>
              <span class="author-block">Xinyue Li<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Yukang Feng<sup style="color:#ffac33; font-size: 0.7em;">3</sup>,</span>
              <span class="author-block">Jianwen Sun<sup style="color:#ffac33; font-size: 0.7em;">3</sup>,</span>
              <span class="author-block">Zizhen Li<sup style="color:#ffac33; font-size: 0.7em;">3</sup>,</span>
              <span class="author-block">Fanrui Zhang<sup style="color:#ffac33; font-size: 0.7em;">3</sup>,</span>
              <span class="author-block">Jiaxin Ai<sup style="color:#ffac33; font-size: 0.7em;">3</sup>,</span>
              <span class="author-block">Zhixiang Wang<sup style="color:#ed4b82; font-size: 0.7em;">5</sup>,</span><br>
              <span class="author-block">Yuwei Wu<sup style="color:#007bff; font-size: 0.7em;">2</sup><sup style="font-size: 0.7em;">,</sup><sup style="color:#9976db; font-size: 0.7em;">4</sup><sup style="font-size: 0.7em;">,</sup><sup style="font-size: 0.7em;">üìß</sup>,</span>
              <span class="author-block">Tong He<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Jiangmiao Pang<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Yu Qiao<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,</span>
              <span class="author-block">Yunde Jia<sup style="color:#9976db; font-size: 0.7em;">4</sup>,</span>
              <span class="author-block">Kaipeng Zhang<sup style="color:#6fbf73; font-size: 0.7em;">1</sup><sup style="font-size: 0.7em;">,</sup><sup style="color:#ffac33; font-size: 0.7em;">3</sup><sup style="font-size: 0.7em;">,</sup><sup style="font-size: 0.7em;">üìß</sup></span>
            </div>

            <div class="is-size-4 publication-authors mt-3 has-text-white">
              <span class="author-block"><sup style="color:#6fbf73; font-size: 0.7em;">1</sup>Shanghai AI Laboratory,</span>
              <span class="author-block"><sup style="color:#007bff; font-size: 0.7em;">2</sup>Beijing Institute of Technology,</span>
              <span class="author-block"><sup style="color:#ffac33; font-size: 0.7em;">3</sup>Shanghai Innovation Institute,</span><br>
              <span class="author-block"><sup style="color:#9976db; font-size: 0.7em;">4</sup>Shenzhen MSU-BIT University,</span>
              <span class="author-block"><sup style="color:#ed4b82; font-size: 0.7em;">5</sup>The University of Tokyo</span>
            </div>

            <div class="is-size-5 publication-authors mt-3 has-text-white">
              <span class="author-block">üìß Corresponding Authors:</span>
              <span class="author-block"><a href="mailto:wuyuwei@bit.edu.cn">wuyuwei@bit.edu.cn</a>,</span>
              <span class="author-block"><a href="mailto:lichuanhao@pjlab.org.cn">lichuanhao@pjlab.org.cn</a>,</span>
              <span class="author-block"><a href="mailto:zhangkaipeng@pjlab.org.cn" class="has-text-white">zhangkaipeng@pjlab.org.cn</a></span>
            </div>
            
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://www.google.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                        <a href="https://www.google.com" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="https://www.google.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fas fa-file-pdf"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://www.google.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Youtube link -->
                <span class="link-block">
                  <a href="https://www.google.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Introduction Video</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <style>
    .container {
      max-width: 1200px;
      margin: 0 auto;
    }
    .hero-video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      overflow: hidden;
      z-index: -1;
    }
    .hero-video video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .hero-video-overlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: radial-gradient(circle at center, rgba(0, 0, 0, 0.35) 0%, rgba(0, 0, 0, 0.8) 90%);
    }
    .hero.is-fullheight {
      min-height: 100vh;
    }
  </style>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-lefted">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration.
              However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world.
              In this paper, we introduce Sekai (meaning "world" in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. 
              It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories.
              Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning "dream" in Japanese).
              We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.               
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Youtube video -->   
  <section class="hero is-small is-small">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <div class="columns is-centered has-text-lefted">
          <div class="column is-four-fifths">
            <h2 class="title is-3 has-text-lefted">Introduction Video</h2>
            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/KMPxsfIvv60" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-lefted">Dataset Overview</h2>
          <div class="content">
            <img src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/images/figure1-compressed.png" alt="pipeline" class="center mb-3">
            <p>In this paper, we introduce Sekai („Åõ„Åã„ÅÑ, meaning "world" in Japanese), a high-quality egocentric worldwide video dataset for world exploration. Most videos contain audio for an immersive world generation. It also benefits other applications, such as video understanding, navigation, and video-audio co-generation. Sekai-Real comprises over 5000 hours of videos collected from YouTube with high-quality annotations. Sekai-Game comprises videos from a realistic video game, with ground-truth annotations. It has five distinct features:
            </p>
            <div class="box">
              <div class="features-list">
                <p class="mb-3">
                  <span class="has-text-weight-bold">1. High-quality and diverse video.</span> All videos are recorded in 720p, featuring diverse weather, various times, and dynamic scenes.
                </p>
                <p class="mb-3">
                  <span class="has-text-weight-bold">2. Worldwide location.</span> Videos span 100 countries and regions, showcasing 750+ cities with diverse cultures, activities, and landscapes.
                </p>
                <p class="mb-3">
                  <span class="has-text-weight-bold">3. Walking and drone view.</span> Beyond walking videos, Seikai includes drone view (FPV and UAV) videos for unrestricted world exploration.
                </p>
                <p class="mb-3">
                  <span class="has-text-weight-bold">4. Long duration.</span> All walking videos are at least 60 seconds long, ensuring real-world, long-term world exploration.
                </p>
                <p class="mb-3">
                  <span class="has-text-weight-bold">5. Rich annotations.</span> All videos are annotated with location, scene, weather, crowd density, captions, and camera trajectories. YouTube videos' annotations are of high quality, while annotations from the game are considered ground truth.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-small">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-lefted">Dataset Curation</h2>
          <div class="content">
            <img src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/images/figure2-compressed.png" alt="pipeline" class="center mb-3">
            <p>
              The Sekai dataset curation pipeline comprises four stages: video collection, pre-processing, annotation, and diverse sampling.
              It gathers over 8600 hours of high-resolution YouTube videos and 70 hours of photorealistic game footage. In pre-processing, videos are segmented into 400,000+ clips, followed by luminance, quality, subtitle, and trajectory filtering to ensure clean, high-quality data. Annotation is powered by LLMs (e.g., Qwen2.5-VL-72B, GPT-4o) and structure from motion models (MegaSaM), covering location, scene categories, detailed captions, and camera trajectories. Finally, a high-quality subset (Sekai-Real-HQ) is sampled using a combination of quality scores and diversity-aware strategies across location, category, content, and trajectory to ensure broad and balanced coverage for training.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-lefted">YUME Model</h2>
          <div class="content">
            <img src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project/static/images/figure3-compressed.png" alt="pipeline" class="center mb-3">
            <p>
              We train an interactive world exploration model named YUME („ÇÜ„ÇÅ, meaning "dream" in Japanese) using a subset of the Sekai-Real-HQ. Specifically, it receives an image and allows unrestricted exploitation using keyboard and mouse control from users.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page is adapted from the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> and licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a> license.

            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>