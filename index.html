<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Sekai: A Video Dataset towards World Exploration">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="video generation, world model">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ùôîùô™ùô¢ùôö</title>
  <link rel="icon" type="image/x-icon"
    href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/css/bulma.min.css">
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/js/fontawesome.all.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/js/bulma-slider.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/hls.js@latest"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var video = document.getElementById('hero-video');
      if (Hls.isSupported()) {
        var hls = new Hls();
        hls.loadSource('./static/videos/banner.m3u8');
        hls.attachMedia(video);
        hls.on(Hls.Events.MANIFEST_PARSED, function () {
          video.play();
        });
      }
      else if (video.canPlayType('application/vnd.apple.mpegurl')) {
        video.src = './static/videos/banner.m3u8';
        video.addEventListener('loadedmetadata', function () {
          video.play();
        });
      }
    });
  </script>
</head>

<body>


  <section class="hero is-fullheight">
    <div class="hero-video">
      <video id="hero-video" autoplay muted loop playsinline>
        <source src="./static/videos/banner.m3u8"
          type="application/x-mpegURL">
      </video>
      <div class="hero-video-overlay"></div>
    </div>
    <div class="hero-body" style="padding-top: 24vh;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title publication-title has-text-white" style="font-size: 3.5rem;white-space: nowrap;">
              <span style="color: #6fbf73;">ùíÄ</span>
              <span style="color: #007bff;">ùëº</span>
              <span style="color: #ffac33;">ùë¥</span>
              <span style="color: #9976db;">ùë¨</span>: An Interactive World Generation Model
            </h1>
            <div class="is-size-4 publication-authors has-text-white">
              <span class="author-block">
                Xiaofeng Mao<sup style="color:#6fbf73; font-size: 0.7em;">1</sup><sup style="font-size: 0.7em;">,</sup><sup style="color:#007bff; font-size: 0.7em;">2</sup>,
              </span>
              <span class="author-block">
                Shaoheng Lin<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span>
              <span class="author-block">
                Zhen Li<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span>
              <span class="author-block">
                Chuanhao Li<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span>
              <span class="author-block">
                Wenshuo Peng<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span><br>
              <span class="author-block">
                Tong He<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span>
              <span class="author-block">
                Jiangmiao Pang<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span>
              <span class="author-block">
                Mingmin Chi<sup style="color:#007bff; font-size: 0.7em;">2</sup><sup style="font-size: 0.7em;">‚Ä°</sup>,
              </span>
              <span class="author-block">
                Yu Qiao<sup style="color:#6fbf73; font-size: 0.7em;">1</sup>,
              </span>
              <span class="author-block">
                Kaipeng Zhang<sup style="color:#6fbf73; font-size: 0.7em;">1</sup><sup style="font-size: 0.7em;">,</sup><sup style="color:#ffac33; font-size: 0.7em;">3</sup><sup style="font-size: 0.7em;">‚Ä†‚Ä°</sup>
              </span>
            </div>
            
            <div class="is-size-4 publication-authors mt-3 has-text-white">
              <span class="author-block"><sup style="color:#6fbf73; font-size: 0.7em;">1</sup>Shanghai AI Laboratory,</span>
              <span class="author-block"><sup style="color:#007bff; font-size: 0.7em;">2</sup>Fudan University,</span>
              <span class="author-block"><sup style="color:#ffac33; font-size: 0.7em;">3</sup>Shanghai Innovation Institute</span>
            </div>
            
            
            <!--             <div class="is-size-6 publication-note mt-3 has-text-white">
              <p>This work was done during Xiaofeng's internship at Shanghai AI Laboratory.<br>
              <sup>‚Ä†</sup> Project Leader<br>
              <sup>‚Ä°</sup> Corresponding Author</p>
            </div>
             -->

            <div class="is-size-5 publication-authors mt-3 has-text-white">
              <span class="author-block">We are looking for collaboration and self-motivated interns. Contact: <a
                  href="mailto:zhangkaipeng@pjlab.org.cn" class="has-text-white">zhangkaipeng@pjlab.org.cn</a>.</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/xxx" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                        <a href="https://www.google.com" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="https://huggingface.co/stdstu123/Yume-I2V-540P" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fas fa-file-pdf"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/stdstu12/YUME" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Youtube link -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=51VII_iJ1EM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Demo Video</span>
                  </a>
                </span>

                <!-- Form link -->
                <span class="link-block">
                  <a href="https://docs.google.com/forms/d/e/1FAIpQLSd5GiQLL1vZQSo0fMDDINd2i_N0rga0a5008Td3lMw9ZimcUQ/viewform?usp=dialog" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-wpforms"></i>
                    </span>
                    <span>Sekai Dataset Access Assistance Form <span class="update-badge">(NEW)</span></span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <style>
    .container {
      max-width: 1200px;
      margin: 0 auto;
    }

    .hero-video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      overflow: hidden;
      z-index: -1;
    }

    .hero-video video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    .hero-video-overlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: radial-gradient(circle at center, rgba(0, 0, 0, 0.35) 0%, rgba(0, 0, 0, 0.8) 90%);
    }

    .hero.is-fullheight {
      min-height: 100vh;
    }

    .update-badge {
    color: red;
    font-weight: bold;
    margin-left: 6px;
    animation: flash 1.5s infinite;
  }

  @keyframes flash {
    0%, 100% { opacity: 1; }
    50% { opacity: 0; }
  }
  </style>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-lefted">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Youtube video -->
  <section class="hero is-small is-small">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <div class="columns is-centered has-text-lefted">
          <div class="column is-four-fifths">
            <h2 class="title is-3 has-text-lefted">Introduction Video</h2>
            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/51VII_iJ1EM" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-lefted">Overview</h2>
          <div class="content">
            <img src="./static/images/yume1.png"
              alt="Yume framework" class="center mb-3">
            <img src="./static/images/yume2_01.png"
              alt="Yume framework" class="center mb-3">
            <p>Yume aims to create <strong>interactive, realistic, and dynamic worlds</strong> from images, text, or videos, enabling exploration and control via peripheral devices. This preview version generates dynamic worlds from images with keyboard-controllable navigation.</p>
            
            <div class="box">
              <h3 class="title is-4">Technical Framework</h3>
              <div class="features-list">
                <p class="mb-4">
                  <span class="has-text-weight-bold has-text-primary">1. Camera Motion Quantization</span><br>
                  <strong>Quantized Camera Motion (QCM)</strong> translates camera trajectories into intuitive directional controls (forward/backward/left/right) and rotational actions (turn right/turn left/tilt up/tilt down) mapped to keyboard input. QCM embeds spatiotemporal context into control signals without additional learnable modules.
                </p>
                
                <p class="mb-4">
                  <span class="has-text-weight-bold has-text-info">2. Video Generation Architecture</span><br>
                  <strong>Masked Video Diffusion Transformer (MVDT)</strong> with frame memory enables infinite autoregressive generation. This overcomes text-based control limitations observed in prior work, maintaining consistency across long sequences.
                </p>
                
                <p class="mb-4">
                  <span class="has-text-weight-bold has-text-warning">3. Enhanced Sampling Mechanisms</span><br>
                  ‚Ä¢ <strong>Anti-Artifact Mechanism (AAM)</strong>: Training-free refinement of latent representations to enhance details<br>
                  ‚Ä¢ <strong>Time-Travel SDE (TTS-SDE)</strong>: Stochastic differential equation-based sampling using future-frame guidance to maintain temporal coherence
                </p>
                
                <p class="mb-4">
                  <span class="has-text-weight-bold has-text-danger">4. Optimization Acceleration</span><br>
                  Synergistic optimization combining <strong>adversarial distillation</strong> and <strong>caching mechanisms</strong> boosts sampling efficiency 3√ó while preserving visual fidelity.
                </p>
              </div>
            </div>
  
            <div class="notification is-primary">
              <p>Trained on the <strong>Sekai world exploration dataset</strong>, Yume achieves remarkable results across diverse scenes. <span class="has-text-weight-bold">All resources are open-source:</span></p>
              <p class="mt-2">
                <span class="icon-text">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>
                    <a href="https://github.com/stdstu12/YUME">
                      https://github.com/stdstu12/YUME
                    </a>
                  </span>
                </span>
              </p>
            </div>
  
          </div>
        </div>
      </div>
    </div>
  </section>
<!-- 
  <section class="section hero is-small">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-lefted">Dataset Curation</h2>
          <div class="content">
            <img src="./static/images/figure2-compressed.png"
              alt="pipeline" class="center mb-3">
            <p>
              The Sekai dataset curation pipeline comprises four stages: video collection, pre-processing, annotation,
              and diverse sampling.
              It gathers over 8600 hours of high-resolution YouTube videos and 40 hours of photorealistic game footage.
              In pre-processing, videos are segmented into 400,000+ clips, followed by luminance, quality, subtitle, and
              trajectory filtering to ensure clean, high-quality data. Annotation is powered by LLMs (e.g.,
              Qwen2.5-VL-72B, GPT-4o) and structure from motion models (MegaSaM), covering location, scene categories,
              detailed captions, and camera trajectories. Finally, a high-quality subset (Sekai-Real-HQ) is sampled
              using a combination of quality scores and diversity-aware strategies across location, category, content,
              and trajectory to ensure broad and balanced coverage for training.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-lefted">YUME Model</h2>
          <div class="content">
            <img src="https://cdn.jsdelivr.net/gh/Lixsp11/sekai-project@0.3.0/static/images/figure3-compressed.png"
              alt="pipeline" class="center mb-3">
            <p>
              We train an interactive world exploration model named YUME („ÇÜ„ÇÅ, meaning "dream" in Japanese) using a
              subset of the Sekai-Real-HQ. Specifically, it receives an image and allows unrestricted exploitation using
              keyboard and mouse control from users.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{li2025sekai,
    title={Sekai: A Video Dataset towards World Exploration}, 
    author={Zhen Li and Chuanhao Li and Xiaofeng Mao and Shaoheng Lin and Ming Li and Shitian Zhao and Zhaopan Xu and Xinyue Li and Yukang Feng and Jianwen Sun and Zizhen Li and Fanrui Zhang and Jiaxin Ai and Zhixiang Wang and Yuwei Wu and Tong He and Jiangmiao Pang and Yu Qiao and Yunde Jia and Kaipeng Zhang},
    journal={arXiv preprint arXiv:2506.15675},
    year={2025}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page is adapted from the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> and licensed under <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a> license.

            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
